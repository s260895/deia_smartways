{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.model_selection import cross_validate, ShuffleSplit\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.model_selection import cross_validate, ShuffleSplit\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.linear_model import LinearRegression\n",
    "import pandas as pd\n",
    "# preprocessing\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler, OneHotEncoder\n",
    "from sklearn.model_selection import train_test_split, cross_validate, ShuffleSplit\n",
    "\n",
    "# models\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.ensemble import RandomForestRegressor \n",
    "from sklearn.neural_network import MLPRegressor\n",
    "\n",
    "\n",
    "#pipelines\n",
    "from sklearn.pipeline import make_pipeline, Pipeline\n",
    "from sklearn.compose import make_column_transformer\n",
    "\n",
    "import pickle5 as pickle\n",
    "import itertools\n",
    "from joblib import dump,load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "def get_occupancy_df(line,cleaned_df = None):\n",
    "    '''calculates the occupancy column for passed dataframe(9292 or arriva) and adds it to that dataframe '''\n",
    "\n",
    "    #CHECK TO ENSURE INPUT CLEANED DATAFRAME IS PASSED AS PART OF THE ARGUMENTS\n",
    "    if cleaned_df is None:\n",
    "        return None\n",
    "    \n",
    "    #LIST OF VALID LINE NUMBERS\n",
    "    # CURRENTLY ONLY WORKING FOR LINE 4 OF BREDA \n",
    "    line_names = ['4']\n",
    "\n",
    "    # CHECK TO ENSURE VALID LINE NUMBER PASSED IN PARAMETERS\n",
    "    if line not in line_names:\n",
    "        print(\"Invalid Line Number\")\n",
    "        return None\n",
    "   \n",
    "    #ORDER OF STOPS BELOW NEED TO MATCH LINE AND MODALITY GIVEN IN THE PARAMETERS IN THIS CASE: 2\n",
    "    line_stops = [['Breda, Dreef','Breda, Nieuwe Heilaarstraat','Breda, Woonboulevard','Breda, Liesboslaan','Breda, Ambachtenlaan',\n",
    "    'Breda, Doelen','Breda, Hovenierstraat','Breda, Burgemeester Sutoriusstraat','Breda, Flierstraat','Breda, Mgr.Nolensplein',\n",
    "    'Breda, Heuvelbrink','Breda, Dr. Struyckenplein','Breda, Bontekoestraat','Breda, Amphia Zkh. Langendijk',\n",
    "    'Breda, Langendijk','Breda, Graaf Hendrik III Laan','Breda, Grote Spie','Breda, Irenestraat',\n",
    "    'Breda, Markendaalseweg','Breda, Centrum','Breda, Vlaszak','Breda, Centraal Station','Breda, Belcrumweg',\n",
    "    'Breda, Konijnenberg','Breda, Spinveld','Breda, Donk','Breda, Heienlangdonk','Breda, Somerweide','Breda, Noortberghmoeren',\n",
    "    'Breda, Cannaertserf','Breda, Komoord','Breda, Dwarsdijk','Breda, Emerparklaan','Breda, Heksenwiellaan']]\n",
    "    \n",
    "     \n",
    "        \n",
    "    #CREATE DICTIONARY OF ORDERED STOPS SO THAT WE CAN MATCH GIVEN SOURCE AND DESTINATION\n",
    "    line_stop_dict = {line_name:line_stop for (line_name,line_stop) in zip(line_names,line_stops)} \n",
    "    for key in line_stop_dict.keys():\n",
    "        line_stop_dict[key] = {stop.lower():num for (num,stop) in enumerate(line_stop_dict[key])}\n",
    "\n",
    "    #INITIALIZE DATAFRAME OBJECT TO BE RETURNED AS THE OCCUPANCY DATAFRAME\n",
    "    occupancy_df = pd.DataFrame(columns = ['Line','Direction','Date','Station','Occupancy'])\n",
    "\n",
    "    #ITERATE OVER THE LINES PARAMTER PASSED TO THIS FUNCTION\n",
    "    #ALL POSSIBLE VALUES: '4'\n",
    "    for line in lines:\n",
    "\n",
    "            #LIST WHICH STORES UNIQUE SOURCE-DESTINATION PAIRS\n",
    "            stops_direction_pair = list()\n",
    "            \n",
    "            # 1: GENERATE ALL POSSIBLE PERMUTATIONS OF LENGTH 2 FROM ALL THE STOPS ON THE CURRENT LINE:\n",
    "            # 2: ITERATE OVER THIS COLLECTION, SEPARATING ALL PAIRS BASED ON DIRECTION (SOURC<DEST:ORIGINAL, SOURC>DEST:REVERSE)\n",
    "            for elem in itertools.permutations(line_stop_dict[line].items(),2):\n",
    "                if elem[0][1] < elem[1][1]:\n",
    "                    stops_direction_pair.append((elem[0],elem[1],'original')) \n",
    "                if elem[0][1] > elem[1][1]:\n",
    "                    stops_direction_pair.append((elem[0],elem[1],'reverse'))\n",
    "            \n",
    "            #ITERATE OVER ALL POSSIBLE DIRECTION VALUES: 'original','reverse'\n",
    "            for direction in directions:\n",
    "                \n",
    "                # SELECT ALL SOURCE-DESTINATION PAIRS WHERE THE DIRECTION MATCHES THE CURRENT DIRECTION \n",
    "                stops= [elem for elem in stops_direction_pair if elem[2]==direction]\n",
    "                \n",
    "                #TEMPORARY DATAFRAME TO STORE THE DATAFRAME SLICE FOR CURRENT OCCUPANCY CALCULATION \n",
    "                temp_df = pd.DataFrame()\n",
    "                \n",
    "                #ITERATE OVER ALL SELECTED SOURCE-DEST PAIRS, APPENDING ALL ROWS FOUND FROM THE CLEANED INPUT DATAFRAME IN THIS MANNER \n",
    "                for elem in stops:\n",
    "                    temp_df = temp_df.append(cleaned_df[(cleaned_df['Source']==elem[0][0]) &(cleaned_df['Destination']==elem[1][0])])\n",
    "                \n",
    "                # ADDING SOURCE NUMBER AND DESTINATION NUMBER COLUMNS DENOTING THE POSITION OF THE STOP IN THE TRIP\n",
    "                temp_df['Source Num'] = [line_stop_dict[line][stop] for stop in temp_df['Source']] \n",
    "                temp_df['Destination Num'] = [line_stop_dict[line][stop] for stop in temp_df['Destination']]\n",
    "                \n",
    "                #ITERATE OVER ALL UNIQUE DATES IN THE SELECTED DATAFRAME\n",
    "                for date in temp_df['Date'].unique():\n",
    "\n",
    "                    # ITERATE OVER ALL UNIQUE LOCATIONS PRESENT IN THE SELECTED DATAFRAME\n",
    "                    for stop in pd.Index(temp_df['Source']).union(pd.Index(temp_df['Destination'])).unique():\n",
    "\n",
    "                        #CALCULATE OCCUPANCY FOR THE SELCTED STOP BASED ON THE DIRECTION OF THE CURRENT SELECTED PAIR\n",
    "                        # CALCULATIONS OF OCCUPANCY ARE DIFFERENT BUT SYMMETRIC FOR BOTH DIRECTIONS\n",
    "                        if direction == 'original':\n",
    "                            occupancy = temp_df[(temp_df['Date'] == date)&( temp_df['Source Num']  <= line_stop_dict[line][stop] )]['Count'].sum() - temp_df[(temp_df['Date'] == date) & (temp_df['Destination Num'] < line_stop_dict[line][stop]) ]['Count'].sum()\n",
    "                        if direction == 'reverse':\n",
    "                            occupancy = temp_df[(temp_df['Date'] == date)&( temp_df['Source Num']  >= line_stop_dict[line][stop] )]['Count'].sum() - temp_df[(temp_df['Date'] == date) & (temp_df['Destination Num'] > line_stop_dict[line][stop]) ]['Count'].sum()\n",
    "\n",
    "                        #APPENDING A ROW TO THE OCCUPANCY DATAFRAME BASED ON THE CURRENT LINE,DIRECTION,DATE,STATION AND STATION NUM\n",
    "                        occupancy_df = occupancy_df.append({'Line':line,'Direction':direction,'Date':date,'Station':stop,'Station Num':int(line_stop_dict[line][stop]),'Occupancy':occupancy},ignore_index=True)\n",
    "\n",
    "    #ADD DAY COLUMN TO OCCUPANCY DATAFRAME\n",
    "    occupancy_df['Day']=pd.Series(pd.DatetimeIndex(occupancy_df['Date'])).dt.day_name()\n",
    "\n",
    "    return occupancy_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_merged_df(occupancy_9292,occupancy_arriva):\n",
    "    '''returns merged dataset after matching 9292 and arriva dataset rows'''\n",
    "    \n",
    "    return pd.merge(occupancy_9292,occupancy_arriva,how='inner',on=['Line','Direction','Date','Day','Station Num','Station'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_candidate_models(x,y,params=None):\n",
    "    '''returns a dictionary of 3 fitted candidate models'''\n",
    "    \n",
    "    if params is None:\n",
    "        return None\n",
    "\n",
    "    regressors = {'Default MLP':MLPRegressor().fit(x.reshape(-1,1),y),\n",
    "              'Tuned MLP': GridSearchCV(MLPRegressor(),params).fit(x.reshape(-1,1),y),\n",
    "              'Linear Regressor': LinearRegression().fit(x.reshape(-1,1),y)\n",
    "             }\n",
    "    return regressors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def regression_scores(x,y,regressors):\n",
    "    ''' returns a dataframe containing r2 validation scores for a list of regressors  '''\n",
    "\n",
    "    results = dict() \n",
    "    \n",
    "    for (model_name,model_pipe) in regressors.items():\n",
    "        for metric in ['r2']: \n",
    "            results[model_name]=np.mean(cross_validate(model_pipe,x.reshape(-1,1), y, scoring=metric,cv=2, return_train_score=False)['test_score'])\n",
    "\n",
    "    res = pd.DataFrame(data=results,index=pd.Series([0]))\n",
    "    res['metrics'] = pd.Series(['r2'])\n",
    "\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def regression_plot(x,y,xlabel='X',ylabel='Y',logy=False,regressors=None):\n",
    "    '''creates a scatter plot of all passed samples along with predictions of of all passed regressors'''\n",
    "\n",
    "    fig, ax = plt.subplots()    \n",
    "    ax.plot(x,y, linewidth=0, marker='v', label='Data points')\n",
    "       \n",
    "    if regressors is not None:\n",
    "\n",
    "        max_x = np.max(x)\n",
    "        for regressor in regressors.items():\n",
    "            #regressor = regressor[1].fit(x.reshape(-1,1),y)\n",
    "            ax.plot(range(int(max_x)), regressor[1].predict(np.array(range(int(max_x))).reshape(-1,1)) ,label=regressor[0])\n",
    "\n",
    "    ax.set_xlabel(xlabel)\n",
    "    ax.set_ylabel(ylabel)\n",
    "    if logy:\n",
    "        ax.set_yscale('log')\n",
    "    ax.legend(facecolor='white')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_occupancy_prediction(line, modality, day,hour,source,destination,merged_df,cleaned_9292,cleaned_arriva,model):\n",
    "    if source == destination:\n",
    "        print('Source and Destination cannot be the same')\n",
    "        return -1 \n",
    "    if line != '4':\n",
    "        print('The MVP supports only line 4 currently')\n",
    "        return -1\n",
    "    \n",
    "    #LINE TO SERVE RESULTS FOR, MUST BE MATCHING DATA IN OCCUPANCY DATAFRAMES GIVEN \n",
    "    line_names = ['4']\n",
    "\n",
    "    #ORDER OF STOPS BELOW NEED TO MATCH LINE AND MODALITY GIVEN IN THE PARAMETERS IN THIS CASE: 2\n",
    "    line_stops = [['Breda, Dreef','Breda, Nieuwe Heilaarstraat','Breda, Woonboulevard','Breda, Liesboslaan','Breda, Ambachtenlaan',\n",
    "    'Breda, Doelen','Breda, Hovenierstraat','Breda, Burgemeester Sutoriusstraat','Breda, Flierstraat','Breda, Mgr.Nolensplein',\n",
    "    'Breda, Heuvelbrink','Breda, Dr. Struyckenplein','Breda, Bontekoestraat','Breda, Amphia Zkh. Langendijk',\n",
    "    'Breda, Langendijk','Breda, Graaf Hendrik III Laan','Breda, Grote Spie','Breda, Irenestraat',\n",
    "    'Breda, Markendaalseweg','Breda, Centrum','Breda, Vlaszak','Breda, Centraal Station','Breda, Belcrumweg',\n",
    "    'Breda, Konijnenberg','Breda, Spinveld','Breda, Donk','Breda, Heienlangdonk','Breda, Somerweide','Breda, Noortberghmoeren',\n",
    "    'Breda, Cannaertserf','Breda, Komoord','Breda, Dwarsdijk','Breda, Emerparklaan','Breda, Heksenwiellaan']]\n",
    "    \n",
    "    #CREATE DICTIONARY OF ORDERED STOPS SO THAT WE CAN MATCH GIVEN SOURCE AND DESTINATION\n",
    "    line_stop_dict = {line_name:line_stop for (line_name,line_stop) in zip(line_names,line_stops)} \n",
    "    for key in line_stop_dict.keys():\n",
    "        line_stop_dict[key] = {stop.lower():num for (num,stop) in enumerate(line_stop_dict[key])}\n",
    "\n",
    "    try:\n",
    "        #GET NUMBER OF SOURCE AND DESTINATION STOPS GIVEN\n",
    "        source_num = line_stop_dict[line][source]\n",
    "        dest_num = line_stop_dict[line][destination]\n",
    "    except:\n",
    "        return {'message': 'error', 'error': 'Please supply valid source and destination stops. For a list of valid values check source code'}\n",
    "\n",
    "\n",
    "    #DEPENDING ON THE DIRECTION OF THE REQUESTED TRIP WE SELECT THE CORRESPONDING DATA\n",
    "    #FOR EXAMPLE FOR source = 2 and destionation = 6 we select stations 2-5 for that day\n",
    "    if source_num < dest_num:\n",
    "        res_df = merged_df[(merged_df['Station Num']>=source_num)&(merged_df['Station Num']<dest_num)&(merged_df['Day']==day)]\n",
    "    if source_num > dest_num:\n",
    "        res_df = merged_df[(merged_df['Station Num']<=source_num)&(merged_df['Station Num']>dest_num)&(merged_df['Day']==day)]\n",
    "        \n",
    "\n",
    "    #9292 request counts for selected stations\n",
    "    requests_count = np.array([res_df['Occupancy_x'][index] for index in res_df.index]).reshape(-1,1)\n",
    "\n",
    "    #number of requests happening for that line, day and hour on average\n",
    "    number_of_requests_hourly = cleaned_9292[(cleaned_9292['Line']==int(line))&(cleaned_9292['Day']==day)&(cleaned_9292['Modaliteitsnummer']==modality)&(cleaned_9292['Hour']==hour)].groupby('Hour').sum()['Count'].mean()\n",
    "    \n",
    "    #number of requests happening for that line and day on average\n",
    "    number_of_requests_daily = cleaned_9292[(cleaned_9292['Line']==int(line))&(cleaned_9292['Day']==day)&(cleaned_9292['Modaliteitsnummer']==modality)].groupby('Hour').sum()['Count'].mean()\n",
    "\n",
    "    #final result is the prediction of the model multiplied by the percentage of requests that happened in that hour\n",
    "    result = np.mean(model.predict(requests_count)) * number_of_requests_hourly/number_of_requests_daily\n",
    "    \n",
    "    #in case no requests were found in that hour the result of the calclulation will be NaN so we return 0\n",
    "    if np.isnan(result):\n",
    "        result = 0\n",
    "\n",
    "    return result\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "ValueError",
     "evalue": "unsupported pickle protocol: 5",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-19-d7809baf4a45>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[1;31m#READ OCCUPANCY DATA FROM PICKLED FILE\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 17\u001b[1;33m \u001b[0moccupancy_df_arriva\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_pickle\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'occupancy_arriva'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     18\u001b[0m \u001b[0moccupancy_df_9292\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_pickle\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'occupancy_9292'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     19\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python37\\site-packages\\pandas\\io\\pickle.py\u001b[0m in \u001b[0;36mread_pickle\u001b[1;34m(filepath_or_buffer, compression)\u001b[0m\n\u001b[0;32m    180\u001b[0m                 \u001b[1;31m# We want to silence any warnings about, e.g. moved modules.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    181\u001b[0m                 \u001b[0mwarnings\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msimplefilter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"ignore\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mWarning\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 182\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mpickle\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    183\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mexcs_to_catch\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    184\u001b[0m             \u001b[1;31m# e.g.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: unsupported pickle protocol: 5"
     ]
    }
   ],
   "source": [
    "# Load 9292 and Arriva Data:\n",
    "cleaned_9292 = pd.read_pickle('final_9292')\n",
    "cleaned_arriva = pd.read_pickle('final_arriva')\n",
    "\n",
    "#RUN THE FOLLOWING LINES TO CREATE THE OCCUPANCY DATASETS IF NEEDED. THE PROCESS TAKES TIME\n",
    "#Create Occupancy Data from 9292 and Arriva Data:\n",
    "# occupancy_df_arriva = get_occupancy_df('4',cleaned_arriva[cleaned_arriva['Line']==4])\n",
    "# occupancy_df_9292 = get_occupancy_df('4',cleaned_9292[(cleaned_9292['Line']==4)&(cleaned_9292['Modaliteitsnummer']==2)])\n",
    "\n",
    "#RUN THE FOLLOWING LINES TO SAVE THE CREATED OCCUPANCY DATASETS IN A PICKLE FORMAT\n",
    "#Store Occupancy Data as pickle:\n",
    "# occupancy_df_arriva.to_pickle('occupancy_arriva')\n",
    "# occupancy_df_9292.to_pickle('occupancy_9292')\n",
    "\n",
    "\n",
    "#READ OCCUPANCY DATA FROM PICKLED FILE\n",
    "occupancy_df_arriva = pd.read_pickle('occupancy_arriva')\n",
    "occupancy_df_9292 = pd.read_pickle('occupancy_9292')\n",
    "\n",
    "#RUN THE FOLLOWING LINES TO CREATE THE MERGED DATASET IF NEEDED.\n",
    "# CREATE MERGED DATA FROM 9292 OCCUPANCY AND ARRIVA OCCUPANCY DATA:\n",
    "#merged_df = get_merged_df(occupancy_df_9292,occupancy_df_arriva)\n",
    "\n",
    "# RUN THE FOLLOWING LINES TO SAVE THE CREATED MERGED DATASET IN A PICKLE FORMAT\n",
    "# STORE MERGED DATA AS PICKLE\n",
    "#merged_df.to_pickle('merged_data')\n",
    "\n",
    "# Read Merged Data from pickle:\n",
    "merged_df = pd.read_pickle('merged_data')\n",
    "\n",
    "# Create Parameter Space for Candidate MLP Models:\n",
    "mlp_params = {  \n",
    "            'hidden_layer_sizes':[(100,),(1000,)],\n",
    "            'activation': ['identity', 'logistic', 'relu', 'tanh'],\n",
    "            # 'mlp__solver':'adam',\n",
    "            'alpha':[0.1,0.01,0.001],\n",
    "            #'learning_rate':['constant'],\n",
    "            'learning_rate_init':[0.1,0.01],\n",
    "            # 'mlp__power_t':0.5,  assert(solver=='sgd' and learning_rate=='inv_scaling)        \n",
    "            'max_iter':[2000]\n",
    "#             'tol':[0.0001],\n",
    "             # 'mlp__verbose':False,\n",
    "            # 'mlp__warm_start':False,\n",
    "            # 'mlp__momentum':0.9,\n",
    "            # 'mlp__nesterovs_momentum':True,\n",
    "            # 'mlp__early_stopping':False,\n",
    "            # 'mlp__validation_fraction':0.05,\n",
    "#             'epsilon':1e-08,\n",
    "#             'beta_1': 0.9,\n",
    "#             'beta_2':0.999,\n",
    "#             'epsilon':1e-8       \n",
    "        }\n",
    "        \n",
    "# Create Trained Candidate Models from Parameter Space:\n",
    "#regressors = get_candidate_models(merged_df['Occupancy_x'].values,merged_df['Occupancy_y'].values,mlp_params)\n",
    "# Store Candidate Models as pickle:\n",
    "#dump(regressors,'candidate_models')\n",
    "# Read Candidate Models from pickle :\n",
    "regressors = load('candidate_models')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "WARNING: pip is being invoked by an old script wrapper. This will fail in a future version of pip.\nPlease see https://github.com/pypa/pip/issues/5599 for advice on fixing the underlying issue.\nTo avoid this problem you can invoke Python with '-m pip' instead of running pip directly.\nERROR: Could not find a version that satisfies the requirement pickle (from versions: none)\nERROR: No matching distribution found for pickle\nWARNING: You are using pip version 20.0.2; however, version 20.3 is available.\nYou should consider upgrading via the 'C:\\ProgramData\\Anaconda3\\python.exe -m pip install --upgrade pip' command.\n"
     ]
    }
   ],
   "source": [
    "!pip install pickle5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "NameError",
     "evalue": "name 'regression_plot' is not defined",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-a1f372f27fc9>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mregression_plot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmerged_df\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'Occupancy_x'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mmerged_df\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'Occupancy_y'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'9292 Request Count'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'Arriva Bus Occupancy'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mregressors\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mregressors\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mlogy\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'regression_plot' is not defined"
     ]
    }
   ],
   "source": [
    "regression_plot(merged_df['Occupancy_x'].values,merged_df['Occupancy_y'].values,'9292 Request Count','Arriva Bus Occupancy',regressors=regressors,logy=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "regression_scores(merged_df['Occupancy_x'].values,merged_df['Occupancy_y'].values,regressors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "44.878249349195634"
      ]
     },
     "metadata": {},
     "execution_count": 16
    }
   ],
   "source": [
    "get_occupancy_prediction(line='4',day='Monday',hour=0,source='breda, centrum',destination='breda, dreef',merged_df=merged_df,cleaned_9292=cleaned_9292,cleaned_arriva=cleaned_arriva,model=regressors['Tuned MLP'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "name": "common-cpu.m59",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/base-cpu:m59"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}